{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9a98eac1-ed17-469c-88c3-bbb4975ee3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "data = pd.read_csv(\"Spam_Email_Data.csv\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# wordnet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_preprocessing(row):\n",
    "    # text cleansing\n",
    "    row = row.lower()\n",
    "    row = re.sub(r'\\b\\S+@\\S+\\b', ' ', row)\n",
    "    row = re.sub(r'<[^>]*>', ' ', row)\n",
    "    row = re.sub(r'[^a-zA-Z\\s]', ' ', row)\n",
    "    # nltk tokenizer\n",
    "    tokens = word_tokenize(row)\n",
    "    # lemmatization\n",
    "    clean_rows = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            clean_token = lemmatizer.lemmatize(token)\n",
    "            clean_rows.append(clean_token)\n",
    "    # join row\n",
    "    clean_row = ' '.join(clean_rows)\n",
    "    return clean_row\n",
    "\n",
    "# apply data preprocessing to each row in data['text']\n",
    "data['clean_text'] = data['text'].apply(data_preprocessing)\n",
    "# print(data['clean_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97bfca7d-0f39-4250-b514-4a864d2ac590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# splitting portion is 60 train - 40 test with random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['target'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7e2a3c60-36ae-4a6d-a06f-504cf08cbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to append models performance in it\n",
    "models_performances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8ffb77f-52c6-4e51-b125-6e2ef445f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf')\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1177364-896f-49f1-a467-36aaa0ae5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tf-idf text embedding technique (not neural networks based)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a58e41ab-a5ab-41ea-99bf-6484ce14622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Tfidf Logistic Regression\n",
      "Accuracy (Tfidf Logistic): 0.9805950840879689\n",
      "Precision (Tfidf Logistic): 0.9957805907172996\n",
      "Recall (Tfidf Logistic): 0.944\n",
      "F1-score (Tfidf Logistic): 0.9691991786447639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# Logistic Regression with regularization parameter C=0.5,set random state to ensure reproducibility\n",
    "logistic_model = LogisticRegression(C=0.5,random_state=42)\n",
    "# fit model to train data\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "# model predict test data\n",
    "y_pred_logistic_tfidf = logistic_model.predict(X_test_tfidf)\n",
    "\n",
    "# Tfidf Logistic Regression Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_tfidf_logistic = \"Tfidf Logistic Regression\"\n",
    "accuracy_tfidf_logistic = accuracy_score(y_test, y_pred_logistic_tfidf)\n",
    "precision_tfidf_logistic = precision_score(y_test, y_pred_logistic_tfidf)\n",
    "recall_tfidf_logistic = recall_score(y_test, y_pred_logistic_tfidf)\n",
    "f1_tfidf_logistic = f1_score(y_test, y_pred_logistic_tfidf)\n",
    "# Print Tfidf Logistic Regression Evaluation Metrics\n",
    "print(\"Model Name:\", model_name)\n",
    "print(\"Accuracy (Tfidf Logistic):\", accuracy_tfidf_logistic)\n",
    "print(\"Precision (Tfidf Logistic):\", precision_tfidf_logistic)\n",
    "print(\"Recall (Tfidf Logistic):\", recall_tfidf_logistic)\n",
    "print(\"F1-score (Tfidf Logistic):\", f1_tfidf_logistic)\n",
    "# Append Tfidf Logistic Regression Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_tfidf_logistic, accuracy_tfidf_logistic, precision_tfidf_logistic, recall_tfidf_logistic, f1_tfidf_logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c222fda-41a9-4e1f-be27-79a94375e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Tfidf SVM\n",
      "Accuracy (Tfidf SVM): 0.9922380336351876\n",
      "Precision (Tfidf SVM): 0.9972826086956522\n",
      "Recall (Tfidf SVM): 0.9786666666666667\n",
      "F1-score (Tfidf SVM): 0.9878869448183042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "# SVM\n",
    "svm_model = SVC(random_state=42)\n",
    "# fit model to train data\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "# model predict test data\n",
    "y_pred_svm_tfidf = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Tfidf SVM Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_tfidf_svm = \"Tfidf SVM\"\n",
    "accuracy_tfidf_svm = accuracy_score(y_test, y_pred_svm_tfidf)\n",
    "precision_tfidf_svm = precision_score(y_test, y_pred_svm_tfidf)\n",
    "recall_tfidf_svm = recall_score(y_test, y_pred_svm_tfidf)\n",
    "f1_tfidf_svm = f1_score(y_test, y_pred_svm_tfidf)\n",
    "# Print Tfidf SVM Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_tfidf_svm)\n",
    "print(\"Accuracy (Tfidf SVM):\", accuracy_tfidf_svm)\n",
    "print(\"Precision (Tfidf SVM):\", precision_tfidf_svm)\n",
    "print(\"Recall (Tfidf SVM):\", recall_tfidf_svm)\n",
    "print(\"F1-score (Tfidf SVM):\", f1_tfidf_svm)\n",
    "# Append Tfidf SVM Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_tfidf_svm, accuracy_tfidf_svm, precision_tfidf_svm, recall_tfidf_svm, f1_tfidf_svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6856da5e-31b0-4adb-ab4c-6fa5f9aad580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words')\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4fb97431-5120-45b6-ab9f-a830db87184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# bag of words text embedding technique (not neural networks based)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e7b03b05-3522-4ccf-83ee-adb55e3afc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Bag of Words Logistic Regression\n",
      "Accuracy (Bag of Words Logistic): 0.9922380336351876\n",
      "Precision (Bag of Words Logistic): 0.9986376021798365\n",
      "Recall (Bag of Words Logistic): 0.9773333333333334\n",
      "F1-score (Bag of Words Logistic): 0.9878706199460917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# Logistic Regression with regularization parameter C=0.5,set random state to ensure reproducibility\n",
    "logistic_model = LogisticRegression(max_iter=1000,C=0.5,random_state=42)\n",
    "# fit model to train data\n",
    "logistic_model.fit(X_train_bow, y_train)\n",
    "# model predict test data\n",
    "y_pred_logistic_bow = logistic_model.predict(X_test_bow)\n",
    "\n",
    "# BOW Logistic Regression Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_bow_logistic = \"Bag of Words Logistic Regression\"\n",
    "accuracy_bow_logistic = accuracy_score(y_test, y_pred_logistic_bow)\n",
    "precision_bow_logistic = precision_score(y_test, y_pred_logistic_bow)\n",
    "recall_bow_logistic = recall_score(y_test, y_pred_logistic_bow)\n",
    "f1_bow_logistic = f1_score(y_test, y_pred_logistic_bow)\n",
    "# Print BOW Logistic Regression Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_bow_logistic)\n",
    "print(\"Accuracy (Bag of Words Logistic):\", accuracy_bow_logistic)\n",
    "print(\"Precision (Bag of Words Logistic):\", precision_bow_logistic)\n",
    "print(\"Recall (Bag of Words Logistic):\", recall_bow_logistic)\n",
    "print(\"F1-score (Bag of Words Logistic):\", f1_bow_logistic)\n",
    "# Append BOW Logistic Regression Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_bow_logistic, accuracy_bow_logistic, precision_bow_logistic, recall_bow_logistic, f1_bow_logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "93dcc85b-fc70-4206-8306-e58f1afb6511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Bag of Words SVM\n",
      "Accuracy (Bag of Words SVM): 0.9870633893919794\n",
      "Precision (Bag of Words SVM): 0.9945054945054945\n",
      "Recall (Bag of Words SVM): 0.9653333333333334\n",
      "F1-score (Bag of Words SVM): 0.9797023004059541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "# SVM\n",
    "svm_model = SVC(random_state=42)\n",
    "# fit model to train data\n",
    "svm_model.fit(X_train_bow, y_train)\n",
    "# model predict test data\n",
    "y_pred_svm_bow = svm_model.predict(X_test_bow)\n",
    "\n",
    "# BOW SVM Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_bow_svm = \"Bag of Words SVM\"\n",
    "accuracy_bow_svm = accuracy_score(y_test, y_pred_svm_bow)\n",
    "precision_bow_svm = precision_score(y_test, y_pred_svm_bow)\n",
    "recall_bow_svm = recall_score(y_test, y_pred_svm_bow)\n",
    "f1_bow_svm = f1_score(y_test, y_pred_svm_bow)\n",
    "# Print BOW SVM Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_bow_svm)\n",
    "print(\"Accuracy (Bag of Words SVM):\", accuracy_bow_svm)\n",
    "print(\"Precision (Bag of Words SVM):\", precision_bow_svm)\n",
    "print(\"Recall (Bag of Words SVM):\", recall_bow_svm)\n",
    "print(\"F1-score (Bag of Words SVM):\", f1_bow_svm)\n",
    "# Append BOW SVM Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_bow_svm, accuracy_bow_svm, precision_bow_svm, recall_bow_svm, f1_bow_svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5c2b297-5e10-4c7b-8cdf-4f8fad4c9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec')\n",
    "print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "727fdbd1-9eca-4ae6-b79a-1f0f30707ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# Word2Vec text embedding technique (neural networks based)\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec([x_train.split() for x_train in X_train])\n",
    "\n",
    "# Function to transform text to word2vec embeddings\n",
    "def word2vec_transform(x_train, model):\n",
    "    word_tokens = x_train.split()\n",
    "    word_vectors = []\n",
    "    for word in word_tokens:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    if word_vectors:\n",
    "        # Take the mean of word vectors to get document vector\n",
    "        text_embedding = np.mean(word_vectors, axis=0)\n",
    "        return text_embedding\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Transform training data to word2vec embeddings\n",
    "X_train_word2vec = []\n",
    "for x_train in X_train:\n",
    "    x_train_embedding = word2vec_transform(x_train, word2vec_model)\n",
    "    X_train_word2vec.append(x_train_embedding)\n",
    "\n",
    "X_train_word2vec = np.array(X_train_word2vec)\n",
    "\n",
    "# Transform test data to word2vec embeddings\n",
    "X_test_word2vec = []\n",
    "for x_test in X_test:\n",
    "    x_test_embedding = word2vec_transform(x_test, word2vec_model)\n",
    "    X_test_word2vec.append(x_test_embedding)\n",
    "\n",
    "X_test_word2vec = np.array(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a9453b98-76c7-4f5b-b3e4-6b17386c097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Word2Vec Logistic Regression\n",
      "Accuracy (Word2Vec Logistic Regression): 0.9849072876239758\n",
      "Precision (Word2Vec Logistic Regression): 0.9917469050894085\n",
      "Recall (Word2Vec Logistic Regression): 0.9613333333333334\n",
      "F1-score (Word2Vec Logistic Regression): 0.976303317535545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# Logistic Regression with regularization parameter C=0.5,set random state to ensure reproducibility\n",
    "logistic_model = LogisticRegression(C=0.5,random_state=42)\n",
    "# fit model to train data\n",
    "logistic_model.fit(X_train_word2vec, y_train)\n",
    "# model predict test data\n",
    "y_pred_logistic_word2vec = logistic_model.predict(X_test_word2vec)\n",
    "\n",
    "# Word2Vec Logistic Regression Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_word2vec_logistic = \"Word2Vec Logistic Regression\"\n",
    "accuracy_word2vec_logistic = accuracy_score(y_test, y_pred_logistic_word2vec)\n",
    "precision_word2vec_logistic = precision_score(y_test, y_pred_logistic_word2vec)\n",
    "recall_word2vec_logistic = recall_score(y_test, y_pred_logistic_word2vec)\n",
    "f1_word2vec_logistic = f1_score(y_test, y_pred_logistic_word2vec)\n",
    "# Print Word2Vec Logistic Regression Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_word2vec_logistic)\n",
    "print(\"Accuracy (Word2Vec Logistic Regression):\", accuracy_word2vec_logistic)\n",
    "print(\"Precision (Word2Vec Logistic Regression):\", precision_word2vec_logistic)\n",
    "print(\"Recall (Word2Vec Logistic Regression):\", recall_word2vec_logistic)\n",
    "print(\"F1-score (Word2Vec Logistic Regression):\", f1_word2vec_logistic)\n",
    "# Append Word2Vec Logistic Regression Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_word2vec_logistic, accuracy_word2vec_logistic, precision_word2vec_logistic, recall_word2vec_logistic, f1_word2vec_logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "52ddca52-8748-47d5-9a9a-e029939158aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Word2Vec SVM\n",
      "Accuracy (Word2Vec SVM): 0.9853385079775765\n",
      "Precision (Word2Vec SVM): 0.9917582417582418\n",
      "Recall (Word2Vec SVM): 0.9626666666666667\n",
      "F1-score (Word2Vec SVM): 0.9769959404600812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# SVM\n",
    "svm_model = SVC(random_state=42)\n",
    "# fit model to train data\n",
    "svm_model.fit(X_train_word2vec, y_train)\n",
    "# model predict test data\n",
    "y_pred_svm_word2vec = svm_model.predict(X_test_word2vec)\n",
    "\n",
    "# Word2Vec SVM Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_word2vec_svm = \"Word2Vec SVM\"\n",
    "accuracy_word2vec_svm = accuracy_score(y_test, y_pred_svm_word2vec)\n",
    "precision_word2vec_svm = precision_score(y_test, y_pred_svm_word2vec)\n",
    "recall_word2vec_svm = recall_score(y_test, y_pred_svm_word2vec)\n",
    "f1_word2vec_svm = f1_score(y_test, y_pred_svm_word2vec)\n",
    "# Print Word2Vec SVM Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_word2vec_svm)\n",
    "print(\"Accuracy (Word2Vec SVM):\", accuracy_word2vec_svm)\n",
    "print(\"Precision (Word2Vec SVM):\", precision_word2vec_svm)\n",
    "print(\"Recall (Word2Vec SVM):\", recall_word2vec_svm)\n",
    "print(\"F1-score (Word2Vec SVM):\", f1_word2vec_svm)\n",
    "# Append Word2Vec SVM Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_word2vec_svm, accuracy_word2vec_svm, precision_word2vec_svm, recall_word2vec_svm, f1_word2vec_svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "725300d4-4749-40a3-87b7-8ea8099d2192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "print('Doc2Vec')\n",
    "print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c2b90fd9-3304-47df-9f6e-77503e9787fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Doc2Vec text embedding technique (neural networks based)\n",
    "# Prepare data for Doc2Vec\n",
    "labeled_data = [\n",
    "    TaggedDocument(words=text.split(), tags=[i]) for i, text in enumerate(X_train)\n",
    "]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=10)\n",
    "model.build_vocab(labeled_data)\n",
    "model.train(labeled_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Transform training data to Doc2Vec embeddings\n",
    "X_train_doc2vec = [model.infer_vector(x_train.split()) for x_train in X_train]\n",
    "\n",
    "# Transform test data to Doc2Vec embeddings\n",
    "X_test_doc2vec = [model.infer_vector(x_test.split()) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da5cd3a7-993e-4792-92d8-1d54d4f50e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Doc2Vec Logistic Regression\n",
      "Accuracy (Doc2Vec Logistic Regression): 0.9616213885295386\n",
      "Precision (Doc2Vec Logistic Regression): 0.9635343618513323\n",
      "Recall (Doc2Vec Logistic Regression): 0.916\n",
      "F1-score (Doc2Vec Logistic Regression): 0.9391660970608339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# Logistic Regression with regularization parameter C=0.5,set random state to ensure reproducibility\n",
    "logistic_model = LogisticRegression(C=0.5,random_state=42)\n",
    "# fit model to train data\n",
    "logistic_model.fit(X_train_doc2vec, y_train)\n",
    "# model predict test data\n",
    "y_pred_logistic_doc2vec = logistic_model.predict(X_test_doc2vec)\n",
    "\n",
    "# Doc2Vec Logistic Regression Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_doc2vec_logistic = \"Doc2Vec Logistic Regression\"\n",
    "accuracy_doc2vec_logistic = accuracy_score(y_test, y_pred_logistic_doc2vec)\n",
    "precision_doc2vec_logistic = precision_score(y_test, y_pred_logistic_doc2vec)\n",
    "recall_doc2vec_logistic = recall_score(y_test, y_pred_logistic_doc2vec)\n",
    "f1_doc2vec_logistic = f1_score(y_test, y_pred_logistic_doc2vec)\n",
    "# Print Doc2Vec Logistic Regression Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_doc2vec_logistic)\n",
    "print(\"Accuracy (Doc2Vec Logistic Regression):\", accuracy_doc2vec_logistic)\n",
    "print(\"Precision (Doc2Vec Logistic Regression):\", precision_doc2vec_logistic)\n",
    "print(\"Recall (Doc2Vec Logistic Regression):\", recall_doc2vec_logistic)\n",
    "print(\"F1-score (Doc2Vec Logistic Regression):\", f1_doc2vec_logistic)\n",
    "# Append Doc2Vec Logistic Regression Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_doc2vec_logistic, accuracy_doc2vec_logistic, precision_doc2vec_logistic, recall_doc2vec_logistic, f1_doc2vec_logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8771f17c-e026-4e11-ae25-1f167887cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Doc2Vec SVM\n",
      "Accuracy (Doc2Vec SVM): 0.9598965071151359\n",
      "Precision (Doc2Vec SVM): 0.9607293127629734\n",
      "Recall (Doc2Vec SVM): 0.9133333333333333\n",
      "F1-score (Doc2Vec SVM): 0.9364319890635681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "# SVM\n",
    "svm_model = SVC(random_state=42)\n",
    "# fit model to train data\n",
    "svm_model.fit(X_train_doc2vec, y_train)\n",
    "# model predict test data\n",
    "y_pred_svm_doc2vec = svm_model.predict(X_test_doc2vec)\n",
    "\n",
    "# Doc2Vec SVM Evaluation Metrics (Accuracy, Recall, Precision, f1-score) \n",
    "model_name_doc2vec_svm = \"Doc2Vec SVM\"\n",
    "accuracy_doc2vec_svm = accuracy_score(y_test, y_pred_svm_doc2vec)\n",
    "precision_doc2vec_svm = precision_score(y_test, y_pred_svm_doc2vec)\n",
    "recall_doc2vec_svm = recall_score(y_test, y_pred_svm_doc2vec)\n",
    "f1_doc2vec_svm = f1_score(y_test, y_pred_svm_doc2vec)\n",
    "# Print Doc2Vec SVM Evaluation Metrics\n",
    "print(\"Model Name:\", model_name_doc2vec_svm)\n",
    "print(\"Accuracy (Doc2Vec SVM):\", accuracy_doc2vec_svm)\n",
    "print(\"Precision (Doc2Vec SVM):\", precision_doc2vec_svm)\n",
    "print(\"Recall (Doc2Vec SVM):\", recall_doc2vec_svm)\n",
    "print(\"F1-score (Doc2Vec SVM):\", f1_doc2vec_svm)\n",
    "# Append Doc2Vec SVM Evaluation Metrics to models_performances List\n",
    "models_performances.append([model_name_doc2vec_svm, accuracy_doc2vec_svm, precision_doc2vec_svm, recall_doc2vec_svm, f1_doc2vec_svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6c4c7fb0-6830-41e6-9de7-363933b9dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Model  Accuracy  Precision    Recall  f1-score\n",
      "0         Tfidf Logistic Regression  0.980595   0.995781  0.944000  0.969199\n",
      "1                         Tfidf SVM  0.992238   0.997283  0.978667  0.987887\n",
      "2  Bag of Words Logistic Regression  0.992238   0.998638  0.977333  0.987871\n",
      "3                  Bag of Words SVM  0.987063   0.994505  0.965333  0.979702\n",
      "4      Word2Vec Logistic Regression  0.984907   0.991747  0.961333  0.976303\n",
      "5                      Word2Vec SVM  0.985339   0.991758  0.962667  0.976996\n",
      "6       Doc2Vec Logistic Regression  0.961621   0.963534  0.916000  0.939166\n",
      "7                       Doc2Vec SVM  0.959897   0.960729  0.913333  0.936432\n"
     ]
    }
   ],
   "source": [
    "# Summarization of all models performances in a models_performances dataframe\n",
    "models_performances_df = pd.DataFrame(models_performances, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"f1-score\"])\n",
    "print(models_performances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94a5dd-858f-4253-bb0d-d4fb1bd31228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
